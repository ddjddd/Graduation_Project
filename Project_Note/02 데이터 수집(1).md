#  02. 데이터 수집(1)

## 1. 웹 크롤링
**1. HTML 불러오기**  
requests 모듈을 통해 웹페이지의 html 소스를 불러온다. requests는 파이썬에서 http 요청을 보내는 기능을 하며, 내장된 urllib 모듈을 편하게 사용하도록 만든 것이다.  

**2. HTML Parsing**  

* Parsing: 컴파일러 또는 번역기가 원시 부호를 기계어로 번역하는 과정의 한 단계로, 각 문장의 문법적인 구성 또는 구문을 분석하는 과정  
* BeautifulSoup: HTML Parsing 라이브러리로서, 몇 개의 메서드만으로 DOM 추출이 가능하다. 즉, 웹페이지의 HTML 소스에서 원하는 값만 얻어낼 수 있게 해준다.  
* request 모듈로 대상 사이트에 GET 요청을 해서 HTML 소스를 받아오고, BeautifulSoup로 원하는 부분만 파싱해서 사용한다.

**3. 필요한 정보만 추출하기**  
html 태그를 이용하여 웹페이지에서 필요한 정보만 추출한다.


## 2. 고려 사항

* 제품 이미지보다는 사용자가 직접 찍어서 올린 음식 이미지
* 각 음식별 최소 100장 이상은 필요
* 하나의 HTML문서에서 필요한 정보를 가져오기엔 이미지 검색 페이지는 한 페이지당 개수가 정해져 있음.  
-> 동적 로딩이 지원되며 브라우저 자동화 기능이 있는 웹 드라이버(Web Driver)를 사용해야 한다.


## 3. 웹 드라이버(Web Driver)
### Selenium
Selenium은 일반적으로 웹앱을 테스트할 때 주로 사용하는 프레임워크이다. 일종의 자동화 프로그램으로 테스터가 일일이 다 만지지 않아도 자동으로 탐색해주고 원하는 정보를 찾도록 도와준다.

### Selenium: Web Driver
Selenium에선 컴퓨터가 직접 웹 브라우저를 띄운 후 코드를 쳐서 동작시킬 수 있도록 webdriver라는 api를 제공한다. 컴퓨터가 webdriver api로 브라우저를 직접 제어할 수 있게 하기 위해선 driver를 로컬에 설치해주어야 한다.  

### 환경 구축
1) selenium 모듈에서 webdriver를 불러온다.  
2) 다운로드 받아 압축을 해제한 drive 파일 경로를 path 변수에 할당한다.  
3) webdriver.Chrome(path)로 chromedriver로 크롬 브라우저를 제어할 수 있는 창을 띄운다.
```
    from selenium import webdriver  
    
    # 드라이버 파일 위치
    path = "/usr/local/bin/chromedriver"    
    driver = webdriver.Chrome(path)

    # webdriver가 google 페이지에 접속하도록 명령
    driver.get('https://www.google.com')

    # 브라우저 창닫기
    driver.close()
```  
    


-------
참고 문서

 * http://www.seleniumhq.org/docs/
 * http://sacko.tistory.com/13
 * https://link.medium.com/k2GQjsrXsR
