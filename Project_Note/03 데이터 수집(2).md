#  03. 데이터 수집(2)

## 웹 크롤러 제작
**0. Workflow**  
1)  driver -> 웹페이지로 이동  
2) 검색창에 키워드를 넣고 검색  
3) 스크롤을 내리며 이미지 스크래핑  
4) 이미지를 리스트에 추가  
5) 다음 페이지로 이동  
6) 마지막 페이지까지 반복  
7) 리스트 이미지들을 로컬에 저장

**1. Import Module**  

    import urllib.request  
    from bs4 import BeautifulSoup  
    from selenium import webdriver  
    from selenium.webdriver.common.keys import Keys



**2. Init**  
드라이버 위치를 지정해준 후, get()을 통해 URL에 접근한다.
    
    CHROMDRIVER_PATH = "/usr/local/bin/chromedriver"
    DATA_PATH = "./images/bulgogi/"
    URL = "https://www.google.com/imghp?hl=en"
    KEYWORD_LIST = "소불고기,소불고기 반찬,소불고기 레시피"

    def __init__(self):  
        self.driver = webdriver.Chrome("/usr/local/bin/chromedriver")
        self.keyword_list = googleImageCrawler.KEYWORD_LIST.split(',')
        self.scrollDown_count = 12
        self.image_num = 0
        self.driver.get(googleImageCrawler.URL)
        self.startCrawling()


**3. 크롤링 시작**  
키워드 개수만큼 검색을 실행한다. searchKeyword() 발생 후 scrollDown()으로 이동하며 이미지를 저장해나간다.

    def startCrawling(self):  
        count = len(self.keyword_list)
        for _ in range(count):
            try:
                if len(self.keyword_list) != 0:
                    self.searchKeyword()
                    self.scrollDown()
                    images = self.scrapingImage()
                    self.saveImage(images)
                    print("{}% complete ...".format(int((_+1) / count * 100)))
                else:
                    self.driver.quit() # quit all browser. close()는 하나만 종료
                    return
            except:
                print("Error")
                continue

**4. 키워드 검색**  
```find_element_by_id()```를 통해 페이지의 단일 element에 접근한다. google.com의 input 요소의 id인 ```lst-ib```에 키워드를 입력한 후 전송한다.

    def searchKeyword(self):
        keyword = self.keyword_list.pop()   # 리스트에서 키워드를 가져오면서 pop
        elem = self.driver.find_element_by_id("lst-ib") # 구글 검색 input의 id
        elem.send_keys(keyword)    # 키워드 입력
        elem.submit()              # 전송

**5. 더보기 버튼**  
더보기(show more results) 버튼의 id인 smb가 있다면 엔터를 눌러 크롤링을 계속한다.  

    def clickMoreButton(self):  
       self.driver.find_element_by_id('smb').send_keys(Keys.ENTER)

**6. 스크롤 내리기**  
scrollDown_count에 해당되는 숫자(turn)만큼 찾았다면 더보기 버튼을 눌러본다.

    # 스크롤 -> 해당 위치에 있는 이미지를 모두 크롤링
    def scrollDown(self):
        for i in range(0, self.scrollDown_count):
            time.sleep(1.5)
            # iteration이 끝났음을 알림
            self.driver.find_element_by_xpath("//body").send_keys(Keys.END)
            if i==2:
                try:
                    # 브라우저에서 다음 페이지로 넘어갈 때 시간이 걸릴 수 있으므로 잠시 대기
                    time.sleep(1)
                except:
                    self.clickMoreButton()
                    continue


**7. 이미지 스크래핑**

    def scrapingImage(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, "html.parser")    # BeautifulSoup으로 html 파싱
        params = []     # 이미지 리스트
        imgList = soup.find_all("img", class_="rg_ic rg_i") # 각 img의 class가 .rg_ic.rg_i

        for img_data in imgList:
            try:
                params.append(img_data["src"])
            except:
                continue

        return params


**8. 이미지 저장**   
request을 통해 이미지 데이터를 가져온다. ```urllib.request.urlretrieve()```은 url이 가리키는 주소에 접근해서 해당 파일을 로컬에 저장(copy)해주는 역할을 한다.

    def saveImage(self, images):
        for image in images:
            # 파일명 지정
            urllib.request.urlretrieve(image, googleImageCrawler.DATA_PATH + str(self.image_num) + ".jpg")
            self.image_num += 1
  


-----------
참고 문서  

* https://docs.python.org/3.6/library/urllib.request.html  
* https://www.crummy.com/software/BeautifulSoup/bs4/doc/
* https://github.com/SMWYG
